[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently the Data Scientist at the Dangote Industries Limited and I also served as the Analytics faculty member with BNet Learning.\n\nResearch Interests\n\nEnvironmental Statistics\nGeocomputation\nMachine Learning\nData Science\nExperimental Design\nMobile Data Collection\n\n\n\nData Stack\n\n\nData Tool Box\n- R | Python | Julia | SQL | Excel | ODK\n\n\n\n\n\n\n\nDevelopment Environments\n- Rstudio | Jupyter | VS Code | Pluto | Literate Programming with Quarto and Rmarkdown\n\n\n\n\n\n\n\nDatabases\n- PostgreSQL | Elasticsearch\n\n\n\n\n\n\n\nBI Tools\n- PowerBI | Looker Studio | Tableau | Kibana\n\n\n\n\n\n\n\nOperating Systems\n- Windows | Ubuntu | CentOS\n\n\n\n\n\n\n\nEducation\n\n\nUniversity of Ibadan\nMSc in Statistics\nAdvisor: Prof. K.O. Obisesan\n\n\nIbadan, Nigeria\nGranted March 2015\n\n\n\nDissertation: Extreme Value Model for Rainfall Distribution\nResearch Area: Environmental Statistics\nObtained PhD Grade\n\n\n\nFederal University of Technology, Akure\nB.Tech in Industrial Mathematics\n\n\nAkure, Nigeria\nGranted September 2010\n\n\n\n\nExperience\n\n\nDangote Industries Limited\n\n\n2022 - present\n\n\nData Scientist\n\nOrchestrated a data pipeline from a third party system.\nImplemented an extensive ETL process.\nContinuous mining of the transformed data.\nDeveloped and monitor ML models using tidymodels methodology in R.\n\n\n\n\nLongbridge Technologies Limited\n\n\n2019 - 2021\n\n\nHead, Business Transformation & Operational Excellence\n\nWorked in a cross-functional environment with various business groups, and end-users to identify, document, and communicate business processes.\nCreated a system to evaluate the success of any adjustments made within the organization and present findings.\nCommunicated strategies and objectives with relevant departments and colleagues.\n\n\n\neHealth Systems Africa\n\n\n2017 - 2018\n\n\nData Scientist\n\nDeveloped models to discover the patterns and information in vast amounts of spatial and non-spatial data across several programs at eHA to support better programmatic decisions, intervention planning and improved information products.\nApplied data mining techniques, performed statistical analysis, and build high quality prediction models that formed core of eHA’s information products on disease surveillance in particular.\n\n\n\nVenture Garden Group\n\n\n2016-2017\n\n\nData Scientist\n\nLead discovery processes with Institute stakeholders to identify the business requirements and the expected outcome.\nConducted advanced data analysis and complex designs algorithm.\nApplied advanced statistical and predictive modeling techniques to build, maintain, and improve on multiple real-time decision systems.\nValidated analysis using scenario modeling.\n\n\n\nComputer Warehouse Group, Plc\n\n\n2013-2016\n\n\nQuality Assurance Analyst\n\nEncouraged factual approach to decision making by providing the management an accurate analysis of people and processes.\nAchieved success in providing standard value-added metrics for business model.\nConducted quality spot checks of project implementations & Services (Software, Hardware, Communication).\nProvided both administrative and analytic support to departments in order to manage critical and people sensitive projects.\nCollaborated with the VP of Sales in the development of sales forecasts and projections.\nSummarized and report performance against sales quotas to all sales personnel in a timely manner.\nProactively prepare and deliver ad hoc customer analysis to sales team members and senior management.\n\n\n\nPractical Sampling International\n\n\n2012-2013\n\n\nData Analyst\n\nSupervised the data collection process of many high profile projects.\nProcessed and analyzed raw data collected from field work.\nImproved the statistical procedure usage and reporting method.\n\n\n\nTeaching\n\nCodementor.io\n\n\nData Analytics Mentor\n\n\n2018-present\n\n\n\n\nBNet Learning\n\n\nData Analytics Faculty\n\n\n2018-present\n\n\n\n\nEduPristine\n\n\nGuest Faculty, Business Analytics\n\n\n2015-2016\n\n\n\n\nAfriHUB Nigeria\n\n\nGuest Faculty IBM SPSS\n\n\n2011-2012"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Customer Lifetime Value Analytics: Case Study\n\n\n\n\n\n\n\nSupply Chain Analytics\n\n\nCustomer Lifetime Value\n\n\nCustomer Acquisition\n\n\n\n\nCustomer lifetime value (CLTV) is the estimated total amount a customer will spend on a business throughout their relationship with that business. It takes into account the revenue generated by the customer as well as the costs associated with acquiring and serving that customer.\n\n\n\n\n\n\nJul 5, 2023\n\n\nOlumide Oyalola\n\n\n\n\n\n\n\n\nAnalysis of Clinical Trial Data\n\n\n\n\n\n\n\nMachine Learning\n\n\nTidymodels\n\n\nClinical Data\n\n\nClassification\n\n\nPredictive Analytics\n\n\nR\n\n\nTidyverse\n\n\n\n\nIn this piece of an experimental project, we will examine factors that could lead to survival in breast cancer patients. Appropriate machine learning algorithm would be deployed to model the dataset using the tidymodels methodology in R.\n\n\n\n\n\n\nMar 3, 2023\n\n\nOlumide Oyalola\n\n\n\n\n\n\n\n\nTelco Customer Churn\n\n\nFocused customer retention programs\n\n\n\n\nMachine Learning\n\n\nTidymodels\n\n\nClinical Data\n\n\nClassification\n\n\nPredictive Analytics\n\n\nR\n\n\nTidyverse\n\n\n\n\nThis experimental project addresses the customer churn in a telecommunication company. Different classification models are considered in the modeling section using the tidymodels methodology in R.\n\n\n\n\n\n\nMar 3, 2023\n\n\nOlumide Oyalola\n\n\n\n\n\n\n\n\nCreating Animated Visualization in R\n\n\n\n\n\n\n\nAnimation\n\n\nVisualization\n\n\nggplot\n\n\nR\n\n\ngganimate\n\n\n\n\nIn this blog post, I would demonstrate how to create an animated visualizations in R using the gapminder dataset from the gapminder package in R.\n\n\n\n\n\n\nJan 26, 2023\n\n\nOlumide Oyalola\n\n\n\n\n\n\n\n\nWorking with the OSRM API\n\n\n\n\n\n\n\nCode\n\n\nAnalysis\n\n\nGIS\n\n\nR\n\n\nRouting\n\n\nOpen Street Map (osrm)\n\n\n\n\nIn this blog post, I would demonstrate how I used the osrm package in R to return the distance and travel time between a destination and different sources\n\n\n\n\n\n\nOct 19, 2022\n\n\nOlumide Oyalola\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Olumide Oyalola",
    "section": "",
    "text": "I am a data professional. I have extensive background working with varied data sets and using advanced analytics to enable business stakeholders to make informed decisions.\nI’ve since had engagements in various sectors spanning IT services, FMCG, Information services, and health interventions. I have a great communication and people management skills. I make time for freelance projects that are engaging and challenging!\nContact me on Codementor"
  },
  {
    "objectID": "posts/customer-lifetime-value-post/index.html",
    "href": "posts/customer-lifetime-value-post/index.html",
    "title": "Customer Lifetime Value Analytics: Case Study",
    "section": "",
    "text": "Customer lifetime value (CLTV) is the estimated total amount a customer will spend on a business throughout their relationship with that business. It takes into account the revenue generated by the customer as well as the costs associated with acquiring and serving that customer. By analyzing the relationship between customer acquisition costs and revenue generated, we can determine which channels are the most cost-effective for acquiring and retaining high-value customers.\nThe given data (available from the link below) includes information about the customer’s channel, cost of acquisition, conversion rate, and revenue generated.\nDownload Data\n\nThe goal is to analyze the CLTV of customers across different channels and identify the most profitable channels for the business."
  },
  {
    "objectID": "posts/customer-lifetime-value-post/index.html#q1",
    "href": "posts/customer-lifetime-value-post/index.html#q1",
    "title": "Customer Lifetime Value Analytics: Case Study",
    "section": "Q1",
    "text": "Q1\n\nWhat’s the cost of acquisition across different channels?\n\n\nNow, let’s examine the cost of acquisition across different channels and identify the most and least profitable channels:\n\n\nCodecustomer_tbl %>% \n  group_by(channel) %>% \n  summarise(Ave_cost = mean(cost, na.rm = TRUE)) %>% \n  ggplot(aes(x = reorder(channel, Ave_cost), y = Ave_cost)) +\n  geom_bar(stat =\"identity\", width = 0.6) +\n  labs(x = \"Channel\",\n       y = \"Average cost of acquisition\",\n       title = \"Average cost of acquisition across channels\") +\n  geom_label_repel(aes(label = round(Ave_cost)))\n\n\n\n\nInsight ℹ️\n\nPaid advertisement is the most expensive channel whereas email marketing is the least expensive channel."
  },
  {
    "objectID": "posts/customer-lifetime-value-post/index.html#q2",
    "href": "posts/customer-lifetime-value-post/index.html#q2",
    "title": "Customer Lifetime Value Analytics: Case Study",
    "section": "Q2",
    "text": "Q2\n\nWhich channels are most and least effective at converting customers?\n\n\nNow, let’s see which channels are most and least effective at converting customers:\n\n\nCodecustomer_tbl %>% \n  group_by(channel) %>% \n  summarise(Ave_rate = mean(conversion_rate, na.rm = TRUE)) %>% \n  ggplot(aes(x = reorder(channel, Ave_rate), y = Ave_rate)) +\n  geom_bar(stat =\"identity\", width = 0.6, fill = \"steelblue\", color = \"white\") +\n  labs(x = \"Channel\",\n       y = \"Average conversion rate\",\n       title = \"Average conversion rate across channels\") +\n  geom_label_repel(aes(label = round(Ave_rate,2)))\n\n\n\n\nInsight ℹ️\n\nSocial media is the most effective channel for converting customers, whereas paid advertising is the least effective."
  },
  {
    "objectID": "posts/customer-lifetime-value-post/index.html#q3",
    "href": "posts/customer-lifetime-value-post/index.html#q3",
    "title": "Customer Lifetime Value Analytics: Case Study",
    "section": "Q3",
    "text": "Q3\n\nWhat are the most and least profitable channels in terms of generating revenue?\n\n\nNow, let’s calculate the total revenue by channel and have a look at the most and least profitable channels in terms of generating revenue:\n\n\nCodecustomer_tbl %>% \n  group_by(channel) %>% \n  summarise(sum_rev = sum(revenue, na.rm = TRUE)) %>%\n  mutate(Prop = sum_rev/sum(sum_rev),\n         channel = factor(channel, c(\"email marketing\", \"referral\",\n                                     \"paid advertising\", \"social media\") )) %>%\n  arrange(Prop, channel) %>% \n  ggplot(aes(x = 2, y = Prop, fill = channel)) +\n  geom_bar(stat =\"identity\", width = 1, color = \"white\") +\n  xlim(0.3, 2.5) +\n  coord_polar(theta = \"y\",  start = 0) +\n  theme_void() +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"\",\n       y = \"\",\n       title = \"Revenue contribution by channels\",\n       fill = \"\") +\n  geom_text_repel(aes(label = paste0(round(Prop*100,1),\"%\")), size = 3, position = position_stack(vjust = 0.5)) +\n    scale_fill_brewer(palette = \"Blues\", direction = -1) +\n    theme(legend.position = \"right\")\n\n\n\n\nInsight ℹ️\nSo, email marketing is the most profitable channel in terms of generating revenue."
  },
  {
    "objectID": "posts/customer-lifetime-value-post/index.html#q4",
    "href": "posts/customer-lifetime-value-post/index.html#q4",
    "title": "Customer Lifetime Value Analytics: Case Study",
    "section": "Q4",
    "text": "Q4\n\nWhat’s the return on investment for each channel?\n\n\nLet’s examine the return on investment (ROI) for each channel\n\n\nCodecustomer_tbl %>% \n  mutate(roi = revenue/cost) %>% \n  group_by(channel) %>% \n  summarise(Ave_ROI = mean(roi, na.rm = TRUE)) %>% \n  ggplot(aes(x= reorder(channel, Ave_ROI), y = Ave_ROI)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\", color = \"white\",\n           width = 0.6) +\n  labs(x = \"Channel\",\n       y = \"Average Return on Investment\",\n       title =  \"Average Return on Investment (ROI) by Channel\")\n\n\n\n\nInsight ℹ️\nThe ROI from email marketing is way higher than all other channels, whereas the ROI from paid advertising is the lowest."
  },
  {
    "objectID": "posts/customer-lifetime-value-post/index.html#q5",
    "href": "posts/customer-lifetime-value-post/index.html#q5",
    "title": "Customer Lifetime Value Analytics: Case Study",
    "section": "Q5",
    "text": "Q5\n\nWhat is the distribution of customer lifetime value from each channel?\n\n\nLet’s calculate the customer lifetime value from each channel.\nBased on the data we have, we can use the formula below to calculate CLTV\n\n\\[ CLTV=(revenue − Cost)∗ \\frac{conversionRate}{cost} \\]\n\nCodecustomer_tbl %>% \n  mutate(CLTV = (revenue - cost)* (conversion_rate/cost)) %>% \n  group_by(channel) %>% \n  summarise(cltv = mean(CLTV, na.rm = TRUE)) %>% \n  ggplot(aes(x = reorder(channel, cltv), y = cltv, fill = channel)) +\n  geom_bar(stat = \"identity\", width = 0.6, show.legend = FALSE) +\n  labs(x = \"Channel\",\n       y = \"CLTV\",\n       title = \"Customer Lifetime Value by Channel\")\n\n\n\n\nInsight ℹ️\nSo, the customer lifetime value from social media and the referral channels are the most significant.\n\nLet’s compare the CLTV distributions of the CLTV across channels\n\n\nCodecustomer_tbl %>% \n  mutate(CLTV = (revenue - cost)* (conversion_rate/cost)) %>% \n  ggplot(aes(x = channel, y = CLTV, fill =  channel)) +\n  geom_boxplot(alpha = 0.5, show.legend = FALSE, color = \"black\") +\n  labs(x =\"Channel\",\n       y = \"CLTV\",\n       title = \"CLTV distribution by Channel\")\n\n\n\n\nInsight ℹ️\nThe boxplot above suggested that there’s a difference in the distribution of the customer lifetime value from different channels."
  },
  {
    "objectID": "posts/customer-lifetime-value-post/index.html#hypothesis-1",
    "href": "posts/customer-lifetime-value-post/index.html#hypothesis-1",
    "title": "Customer Lifetime Value Analytics: Case Study",
    "section": "Hypothesis 1",
    "text": "Hypothesis 1\n\nNull hypothesis: There’s no significant difference in the revenue generated by channels\nAlternative Hypothesis: There’s a significant different in the revenue generated by channels\n\n\nCodemodel1 <- aov(lm(revenue ~ channel, data = customer_tbl))\nsummary(model1)\n\n             Df     Sum Sq Mean Sq F value Pr(>F)\nchannel       3    3476233 1158744    0.73  0.534\nResiduals   796 1264097594 1588062               \n\n\nInsight ℹ️\n\nThe test result above suggested that there is no significant difference in the revenue across channel at 5% level of significant. As a result, we cannot reject the null hypothesis."
  },
  {
    "objectID": "posts/customer-lifetime-value-post/index.html#hypothesis-2",
    "href": "posts/customer-lifetime-value-post/index.html#hypothesis-2",
    "title": "Customer Lifetime Value Analytics: Case Study",
    "section": "Hypothesis 2",
    "text": "Hypothesis 2\n\nNull Hypothesis: There’s no significant difference in the CLTV across channels\nAlternative Hypothesis: There’s a significant difference in the CLTV across channels\n\n\nCodecustomer_tbl %<>% \n  mutate(CLTV = (revenue - cost)* (conversion_rate/cost))\n\nmodel2 <- aov(lm(CLTV ~ channel, data = customer_tbl))\nsummary(model2)\n\n             Df Sum Sq Mean Sq F value              Pr(>F)    \nchannel       3 237204   79068   339.6 <0.0000000000000002 ***\nResiduals   796 185312     233                                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInsight ℹ️\n\nThe test result above suggest that there’s a significant difference in the customer lifetime value across channels. The test is significant at 5% level of significance. As a result, the null hypothesis cannot be accepted.\n\n\nAs a follow-up question, it would be great to know where the difference lies. Below is a post-hoc test conducted to know where the difference lies.\n\n\nCodeout <- duncan.test(model2, \"channel\", main = \"CLTV across channels\")\nplot(out, variation = \"IQR\", main = \"CLTV across channels\")\n\n\n\nCodeprint(out$groups)\n\n                      CLTV groups\nsocial media     46.584325      a\nreferral         40.599817      b\nemail marketing  23.559595      c\npaid advertising  1.500682      d\n\n\n\nThe post hoc test shows that there’s a significant different across all the channels at 5% level of significance."
  },
  {
    "objectID": "posts/osrm-post/index.html",
    "href": "posts/osrm-post/index.html",
    "title": "Working with the OSRM API",
    "section": "",
    "text": "An interface between R and the OSRM (OpenStreetMap-Based Routing Service) API. OSRM is a routing service based on OpenStreetMap data. See for more information. This package allows to compute routes, trips, isochrones and travel distances matrices (travel time in minutes and distance in kilometer).\nThe package is available on CRAN and it can be installed by running the code chunk below from the Rstudio console.\nFor the purpose of this post, the osrmTable function from the osrm package in R would be used to return the distance in meter and the travel time in minutes.\n\nCode#install.packages(\"osrm\")"
  },
  {
    "objectID": "posts/osrm-post/index.html#load-packages",
    "href": "posts/osrm-post/index.html#load-packages",
    "title": "Working with the OSRM API",
    "section": "Load Packages",
    "text": "Load Packages\n\nCode# Install pacman package if needed\n#if(!require(\"pacman\")) install.packages(\"pacman\")\n\n# load the required packages\n\npacman::p_load(\n  httr,\n  jsonlite,\n  tidyjson,\n  tidyverse,\n  lubridate,\n  geosphere,\n  anytime,\n  tictoc,\n  stringi,\n  maptools,\n  geosphere,\n  sf,\n  sp,\n  openxlsx,\n  leaflet,\n  magrittr,\n  janitor,\n  arrow,\n  osrm\n)\n\n\nLoad Data\n\nCode# load  file as tibble\n\ntic(\"Load the csv file\")\n\ndf <- read_parquet(\"ml_data.parquet\")\n\ntoc()\n\nLoad the csv file: 0.18 sec elapsed"
  },
  {
    "objectID": "posts/osrm-post/index.html#data-munging",
    "href": "posts/osrm-post/index.html#data-munging",
    "title": "Working with the OSRM API",
    "section": "Data Munging",
    "text": "Data Munging\n\nCoderemove <- c(\"/Date\", \"(\", \")/\", \"[[:punct:]]\")\n\nvar <- c(\"ActualSpeed\", \"Address\", \"Altitude\", \"AssetClass\", \"AssetLocationID\", \"AssetStatus\", \"CategoryID\", \"CategoryName\", \"City\", \"CustomerName\", \"DateTimeLocal\", \"DateTimeReceived\", \"DeliveryOrderNumber\", \"DepartureDateTime\", \"DestinationArea\", \"DestinationCity\", \"DestinationSite\", \"DestinationStreet\", \"DeviceType\", \"DirectionString\", \"Distance\", \"DriverCode\", \"DriverID\", \"Geofence\", \"IgnitionStatus\", \"Information\", \"JourneyDistance\", \"JourneyDuration\", \"JourneyIdleTime\", \"JourneyMaxSpeed\", \"LastIgnitionOff\", \"LastIgnitionOn\", \"Latitude\", \"Load\", \"Longitude\", \"NumSatellites\", \"Odometer\", \"Reason\", \"ReasonString\", \"Reference\", \"Region\", \"SiteName\", \"SpeedOverGround\", \"Status\", \"Street\", \"TrackTrue\", \"TripDestination\", \"TripID\", \"TripSource\", \"TripType\", \"UTCDateTime\", \"WaybillNumber\")\n\n\n\nCode# data cleaning\n\ndf %<>% \n  #dplyr::select(tidyselect::all_of(var)) %>% \n  mutate(DateTimeLocal = str_remove_all(`DateTimeLocal`, \n                                        paste(remove, collapse = \"|\")),\n         DateTimeLocal = str_remove_all(`DateTimeLocal`, \n                                        \"\\\\+0100|\\\\+0000\"),\n         DateTimeLocal = as.numeric(`DateTimeLocal`),\n         DateTimeLocal = `DateTimeLocal`/1000,\n         DateTimeLocal = anytime(`DateTimeLocal`),\n         DateTimeReceived = str_remove_all(DateTimeReceived, \n                                           paste(remove, collapse = \"|\")),\n         DateTimeReceived = str_remove_all(DateTimeReceived, \n                                           \"\\\\+0100|\\\\+0000\"),\n         DateTimeReceived = as.numeric(DateTimeReceived),\n         DateTimeReceived = DateTimeReceived/1000,\n         DateTimeReceived = anytime(DateTimeReceived),\n         DepartureDateTime = str_remove_all(DepartureDateTime, \n                                            paste(remove, collapse = \"|\")),\n         DepartureDateTime = str_remove_all(DepartureDateTime, \n                                            \"\\\\+0100|\\\\+0000\"),\n         DepartureDateTime = as.numeric(DepartureDateTime),\n         DepartureDateTime = DepartureDateTime/1000,\n         DepartureDateTime = anytime(DepartureDateTime),\n         UTCDateTime = str_remove_all(UTCDateTime, \n                                      paste(remove, collapse = \"|\")),\n         UTCDateTime = str_remove_all(UTCDateTime, \n                                      \"\\\\+0100|\\\\+0000\"),\n         UTCDateTime = as.numeric(UTCDateTime),\n         UTCDateTime = UTCDateTime/1000,\n         UTCDateTime = anytime(UTCDateTime),\n         LastIgnitionOff = str_remove_all(LastIgnitionOff, \n                                          paste(remove, collapse = \"|\")),\n         LastIgnitionOff = str_remove_all(LastIgnitionOff, \n                                          \"\\\\+0100|\\\\+0000\"),\n         LastIgnitionOff = as.numeric(LastIgnitionOff),\n         LastIgnitionOff = LastIgnitionOff/1000,\n         LastIgnitionOff = anytime(LastIgnitionOff),\n         LastIgnitionOn = str_remove_all(LastIgnitionOn, \n                                         paste(remove, collapse = \"|\")),\n         LastIgnitionOn = str_remove_all(LastIgnitionOn, \n                                         \"\\\\+0100|\\\\+0000\"),\n         LastIgnitionOn = as.numeric(LastIgnitionOn),\n         LastIgnitionOn = LastIgnitionOn/1000,\n         LastIgnitionOn = anytime(LastIgnitionOn)) %>% \n  filter(!between(TripID, 9000000000, Inf)) %>% \n  filter(!TripID == 0)\n\ndf %<>% mutate(TripID = as.numeric(TripID))\n\n\n\nCodeoptions(scipen = 999)\n\n# destination dataframe\n\nibese_df <- data.frame(id = \"ibese\", lon = c(3.043568), lat = c(7.006293))\n\n\nibese <- matrix(c(3.043568,7.006293), ncol = 2)\n\nibese_geofence <- c(\"Ibese\", \"IBESE\", \"Vehicle Park\", \"Vehicle Park 2\")\n\n\nDetermining the direction of the truck\n\nCodeinbound_df <- df %>% \n  filter(AssetStatus == \"InService\", Longitude > 0, \n         Latitude > 0) %>% \n  filter(!between(TripID, 9000000000, Inf)) %>% \n  filter(TripID != 0) %>% \n  select(Reference, DateTimeReceived, \n         Longitude, Latitude, TripID, Geofence, Altitude) %>% \n  arrange(Reference, DateTimeReceived) %>% \n  group_by(Reference, TripID) %>% \n  mutate(.after = DateTimeReceived,\n         TimeDiff = as.numeric(difftime(DateTimeReceived, \n                                        lag(DateTimeReceived, \n                                            default = first(DateTimeReceived)), \n                                        units = \"hours\")),\n         LongLat = matrix(c(Longitude, Latitude), ncol = 2),\n         DistCovered = distGeo(LongLat, lag(LongLat)),\n         DistCovered = DistCovered/1000,\n         DistToPlant = distGeo(LongLat, ibese),\n         DistToPlant = DistToPlant/1000) %>%\n  mutate(Direction = if_else(lag(DistToPlant) < DistToPlant, \"Inbound\", \"Outbound\")) %>% \n  filter(Direction == \"Inbound\") %>% \n  slice_tail(n = 1) %>% \n  filter(TripID != 3000071349)\n\n\nEstimating the arrival time and distance\n\nCodeduration <- osrmTable(src = df %>%\n                        # filter(as.Date(DateTimeReceived) == today()) %>% \n                        select(TripID, Longitude, Latitude) %>% \n                        slice_head(n = 100),\n                      dst = ibese_df, measure = \"duration\", osrm.profile = \"car\")\n\n\ndistance <- osrmTable(src = df %>%\n                        # filter(as.Date(DateTimeReceived) == today()) %>% \n                        select(TripID, Longitude, Latitude) %>% \n                        slice_head(n = 100),\n                      dst = ibese_df, measure = \"distance\", osrm.profile = \"car\")\n\n\ndistances <- distance$distances %>% \n  as.data.frame() %>%\n  rownames_to_column() %>% \n  rename(TripID = rowname,\n         Distance = ibese) %>% \n  mutate(TripID = as.numeric(TripID))\n\n\ndurations <- duration$durations %>% \n  as.data.frame() %>%\n  rownames_to_column() %>% \n  rename(TripID = rowname,\n         duration = ibese) %>% \n  mutate(TripID = as.numeric(TripID))"
  },
  {
    "objectID": "posts/osrm-post/index.html#data-join",
    "href": "posts/osrm-post/index.html#data-join",
    "title": "Working with the OSRM API",
    "section": "Data Join",
    "text": "Data Join\nJoining the durations, distances and inbound_df data frames.\n\nCodearrival <- df %>%\n  inner_join(durations, by = 'TripID') %>% \n  inner_join(distances, by = 'TripID') %>% \n  mutate(Arrival = DateTimeReceived + minutes(round(duration))) %>% \n  select(TripID, Reference, Longitude, Latitude, Distance, DateTimeReceived, Arrival) %>% \n  arrange(Distance)\n\n#DT::datatable(arrival)"
  },
  {
    "objectID": "posts/plot-animation-post/index.html",
    "href": "posts/plot-animation-post/index.html",
    "title": "Creating Animated Visualization in R",
    "section": "",
    "text": "Static visualizations that are publication ready are readily available in R using the ggplot2 package. However, there are times where it’s required to illustrate the change in an event overtime. This is a particular use case of the gganimate package in R which is an extension of the ggplot2 package for creating animated ggplots.\nIt provides a range of functionalities that can be added to the plot object in order to customize how it should change with time.\nThe gganimate package is available on CRAN and it can be installed by running the code chunk below from the Rstudio console.\n\nCodeinstall.packages(\"gganimate\")\n\n\n\nThe gapminder dataset in R is an excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country.\nThe gapminder package is available on CRAN and it can be installed by running the code chunk below from the Rstudio console.\n\nCodeinstall.packages(\"gapminder\")"
  },
  {
    "objectID": "posts/plot-animation-post/index.html#load-packages",
    "href": "posts/plot-animation-post/index.html#load-packages",
    "title": "Creating Animated Visualization in R",
    "section": "Load Packages",
    "text": "Load Packages\n\nCodeif(!require(pacman)) install.packages(\"pacman\")\n\nLoading required package: pacman\n\n\nWarning: package 'pacman' was built under R version 4.0.5\n\nCodepacman::p_load(\n  tidyverse,\n  gganimate,\n  gapminder\n)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/plot-animation-post/index.html#load-demo-dataset",
    "href": "posts/plot-animation-post/index.html#load-demo-dataset",
    "title": "Creating Animated Visualization in R",
    "section": "Load Demo Dataset",
    "text": "Load Demo Dataset\n\nCodedata(gapminder)\n\n\nA closer look at the dataset\n\nIf you’re new to the gapminder dataset, below is the structure of the dataset which includes the variables and sample data.\n\n\nCodeglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   <fct> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", ~\n$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, ~\n$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, ~\n$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8~\n$ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12~\n$ gdpPercap <dbl> 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, ~"
  },
  {
    "objectID": "posts/plot-animation-post/index.html#static-plot",
    "href": "posts/plot-animation-post/index.html#static-plot",
    "title": "Creating Animated Visualization in R",
    "section": "Static Plot",
    "text": "Static Plot\n\nCodep <- ggplot(\n  gapminder, \n  aes(x = gdpPercap, y=lifeExp, size = pop, colour = country)\n  ) +\n  geom_point(show.legend = FALSE, alpha = 0.7) +\n  scale_color_viridis_d() +\n  scale_size(range = c(2, 12)) +\n  scale_x_continuous(labels = scales::comma) +\n  labs(x = \"GDP per capita\", y = \"Life expectancy\")\np"
  },
  {
    "objectID": "posts/plot-animation-post/index.html#transition-through-distinct-states-in-time",
    "href": "posts/plot-animation-post/index.html#transition-through-distinct-states-in-time",
    "title": "Creating Animated Visualization in R",
    "section": "Transition through distinct states in time",
    "text": "Transition through distinct states in time\nBasics\nKey R function: transition_time().\n\nThe transition length between the states will be set to correspond to the actual time difference between them.\n\nLabel variables: frame_time.\n\nGives the time that the current frame corresponds to.\n\n\nCodep + transition_time(year) +\n  labs(title = \"Year: {frame_time}\")\n\nWarning: No renderer available. Please install the gifski, av, or magick\npackage to create animated output\n\n\nNULL\n\n\nCreate facets by continent:\n\nCodep + facet_wrap(~continent) +\n  transition_time(year) +\n  labs(title = \"Year: {frame_time}\")\n\nWarning: No renderer available. Please install the gifski, av, or magick\npackage to create animated output\n\n\nNULL\n\n\nLet the view follow the data in each frame\n\nCodep + transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  view_follow(fixed_y = TRUE)\n\nWarning: No renderer available. Please install the gifski, av, or magick\npackage to create animated output\n\n\nNULL\n\n\n\nCodepop_ng <- gapminder %>% \n  filter(country == \"Nigeria\") %>% \n  ggplot(aes(x = year, y = pop, fill = pop)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  scale_fill_distiller(palette = \"Reds\", direction = 1) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    panel.grid.major.y = element_line(color = \"white\"),\n    panel.ontop = TRUE\n  ) +\n  scale_y_continuous(labels = scales::unit_format(unit = \"M\", scale = 1e-6)) +\n  geom_text(aes(label = paste0(round(pop/1000000,1), \"M\")))\n\npop_ng\n\n\n\n\n\nCodepop_ng + transition_time(year) +\n  shadow_mark() +\n  labs(x = \"Year\",\n       y = \"Population\",\n       title = \"Year: {frame_time}\")\n\nWarning: No renderer available. Please install the gifski, av, or magick\npackage to create animated output\n\n\nNULL"
  },
  {
    "objectID": "posts/plot-animation-post/index.html#save-animation",
    "href": "posts/plot-animation-post/index.html#save-animation",
    "title": "Creating Animated Visualization in R",
    "section": "Save Animation",
    "text": "Save Animation\nIf you need to save the animation for later use you can use the anim_save() function."
  },
  {
    "objectID": "posts/plot-animation-post/index.html#read-more",
    "href": "posts/plot-animation-post/index.html#read-more",
    "title": "Creating Animated Visualization in R",
    "section": "Read more",
    "text": "Read more\n\ngganimate package official documentation"
  },
  {
    "objectID": "posts/survival-tidymodels-post/index.html",
    "href": "posts/survival-tidymodels-post/index.html",
    "title": "Analysis of Clinical Trial Data",
    "section": "",
    "text": "Breast cancer is a disease in which cells in the breast grow out of control. There are different kinds of breast cancer. The kind of breast cancer depends on which cells in the breast turn into cancer. Breast cancer can begin in different parts of the breast. Medical professionals often opined that earlier detection of breast cancer is key to survival.\nIn this piece of an experimental project, we will examine factors that could lead to survival in breast cancer patients. Appropriate machine learning algorithm would be deployed to model the dataset using the tidymodels methodology in R.\n\n\nCode#if(!require(pacman)) install.packages(\"pacman\")\n\npacman::p_load(\n  tidyverse,\n  magrittr,\n  reactable,\n  ggthemes,\n  DescTools,\n  tidymodels,\n  vip\n)\n\noptions(scipen = 999, digits = 2)"
  },
  {
    "objectID": "posts/survival-tidymodels-post/index.html#data-wrangling",
    "href": "posts/survival-tidymodels-post/index.html#data-wrangling",
    "title": "Analysis of Clinical Trial Data",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nConvert the dependent variable survival to factor.\n\nCode# Convert the dependent variable `survival` to factor\n\nsurvival_tbl %<>%\n  mutate(survival = if_else(survival == 1, \"The patient survived 5 years or longer\", \"The patient died within 5 years\"),\n         survival = as.factor(survival))"
  },
  {
    "objectID": "posts/survival-tidymodels-post/index.html#data-quality",
    "href": "posts/survival-tidymodels-post/index.html#data-quality",
    "title": "Analysis of Clinical Trial Data",
    "section": "Data Quality",
    "text": "Data Quality\nCheck dataframe for NAs\n\nCodeany(is.na(survival_tbl))\n\n[1] FALSE\n\n\n\nNo NA is found. The dataset is complete without any missing values.\n\n\nCode# split data to train and test set\n\nset.seed(1234)\n\nsplit <- survival_tbl %>% \n  initial_split(prop = 0.75, strata = survival) # 75% training set | 25% testing set\n\ndf_train <- split %>% \n  training()\n\ndf_test <- split %>% \n  testing()"
  },
  {
    "objectID": "posts/survival-tidymodels-post/index.html#model-recipe",
    "href": "posts/survival-tidymodels-post/index.html#model-recipe",
    "title": "Analysis of Clinical Trial Data",
    "section": "Model Recipe",
    "text": "Model Recipe\n\nCoderec <- recipe(survival ~ ., data = df_train)\n\n# add preprocessing\n\nprepro <- rec %>% \n  step_normalize(all_numeric_predictors()) %>% \n  prep()\n\nprepro"
  },
  {
    "objectID": "posts/survival-tidymodels-post/index.html#define-the-model-with-parsnip",
    "href": "posts/survival-tidymodels-post/index.html#define-the-model-with-parsnip",
    "title": "Analysis of Clinical Trial Data",
    "section": "Define the model with parsnip",
    "text": "Define the model with parsnip\n\nCode## Logistic Regression\n\nlr <- logistic_reg(\n  mode = \"classification\"\n) %>% \n  set_engine(\"glm\")"
  },
  {
    "objectID": "posts/survival-tidymodels-post/index.html#define-models-workflow",
    "href": "posts/survival-tidymodels-post/index.html#define-models-workflow",
    "title": "Analysis of Clinical Trial Data",
    "section": "Define models workflow",
    "text": "Define models workflow\n\nCode## Logistic Regression\n\nlr_wf <- workflow() %>% \n  add_recipe(prepro) %>% \n  add_model(lr)"
  },
  {
    "objectID": "posts/survival-tidymodels-post/index.html#model-fitting",
    "href": "posts/survival-tidymodels-post/index.html#model-fitting",
    "title": "Analysis of Clinical Trial Data",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nCodeset.seed(1234)\n\n## Logistic Regression\n\nlr_wf %>% \n  fit(df_train) %>% \n tidy()\n\n# A tibble: 4 x 5\n  term           estimate std.error statistic    p.value\n  <chr>             <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)      1.29       0.283     4.57  0.00000483\n2 Age             -0.147      0.286    -0.515 0.606     \n3 Operation_year  -0.0605     0.278    -0.218 0.828     \n4 nr_of_nodes     -1.09       0.324    -3.37  0.000745"
  },
  {
    "objectID": "posts/survival-tidymodels-post/index.html#obtaining-predictions",
    "href": "posts/survival-tidymodels-post/index.html#obtaining-predictions",
    "title": "Analysis of Clinical Trial Data",
    "section": "Obtaining Predictions",
    "text": "Obtaining Predictions\n\nCodeset.seed(1234)\n\n## Logistic Regression\n\nlr_pred <- lr_wf %>% \n  fit(df_train) %>% \n  predict(df_test) %>% \n  bind_cols(df_test)"
  },
  {
    "objectID": "posts/survival-tidymodels-post/index.html#evaluating-model-performance",
    "href": "posts/survival-tidymodels-post/index.html#evaluating-model-performance",
    "title": "Analysis of Clinical Trial Data",
    "section": "Evaluating model performance",
    "text": "Evaluating model performance\n\n\nkap: Kappa\n\nsens: Sensitivity\n\nspec: Specificity\n\nf_meas: F1\n\nmcc: Matthews correlation coefficient\n\nLogistic Regression\n\nCodelr_pred %>% \n  conf_mat(truth = survival, estimate = .pred_class) %>% \n  autoplot(type = \"heatmap\")\n\n\n\nCodelr_pred %>% \n  conf_mat(truth = survival, estimate = .pred_class) %>% \n  summary()\n\n# A tibble: 13 x 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.710\n 2 kap                  binary         0.136\n 3 sens                 binary         0.25 \n 4 spec                 binary         0.870\n 5 ppv                  binary         0.4  \n 6 npv                  binary         0.769\n 7 mcc                  binary         0.142\n 8 j_index              binary         0.120\n 9 bal_accuracy         binary         0.560\n10 detection_prevalence binary         0.161\n11 precision            binary         0.4  \n12 recall               binary         0.25 \n13 f_meas               binary         0.308"
  },
  {
    "objectID": "posts/survival-tidymodels-post/index.html#roc-curve-and-auc-estimate",
    "href": "posts/survival-tidymodels-post/index.html#roc-curve-and-auc-estimate",
    "title": "Analysis of Clinical Trial Data",
    "section": "Roc Curve and AUC estimate",
    "text": "Roc Curve and AUC estimate\n\nCodeprob_preds <- lr_wf %>% \n  fit(df_train) %>% \n  predict(df_test, type = \"prob\") %>% \n  bind_cols(df_test)\n\n\nthreshold_df <- prob_preds %>% \n  roc_curve(truth = survival, estimate = `.pred_The patient survived 5 years or longer`)\n\nthreshold_df %>% \n  autoplot()\n\n\n\nCoderoc_auc(prob_preds, truth = survival, estimate = `.pred_The patient survived 5 years or longer`)\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.402"
  },
  {
    "objectID": "posts/survival-tidymodels-post/index.html#variable-importance-plot",
    "href": "posts/survival-tidymodels-post/index.html#variable-importance-plot",
    "title": "Analysis of Clinical Trial Data",
    "section": "Variable Importance Plot",
    "text": "Variable Importance Plot\nRelative variable importance plot\n\nCodefinal_lr_model <-\n  lr_wf %>%\n  fit(data = df_train)\n\nfinal_lr_model\n\n== Workflow [trained] ==========================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n1 Recipe Step\n\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n   (Intercept)             Age  Operation_year     nr_of_nodes  \n        1.2929         -0.1472         -0.0605         -1.0910  \n\nDegrees of Freedom: 88 Total (i.e. Null);  85 Residual\nNull Deviance:      97 \nResidual Deviance: 81   AIC: 89\n\nCodefinal_lr_model %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 4 x 5\n  term           estimate std.error statistic    p.value\n  <chr>             <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)      1.29       0.283     4.57  0.00000483\n2 Age             -0.147      0.286    -0.515 0.606     \n3 Operation_year  -0.0605     0.278    -0.218 0.828     \n4 nr_of_nodes     -1.09       0.324    -3.37  0.000745  \n\nCode## variable importance plot\n\nfinal_lr_model %>%\n  extract_fit_parsnip() %>%\n  vip() +\n  labs(title = 'Variables relative importance',\n       x = \"\") +\n  theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16),\n        axis.title = element_text(family = \"sans\", size = 10, face = \"plain\")) +\n  theme_clean() +\n  scale_y_continuous(labels = scales::comma)"
  },
  {
    "objectID": "posts/telco-churn-tidymodels-post/index.html",
    "href": "posts/telco-churn-tidymodels-post/index.html",
    "title": "Telco Customer Churn",
    "section": "",
    "text": "The ultimate goals of any business enterprise is to maximize profit, minimize cost, ensure efficiency in service delivery among others. In order to achieve this, the business ensures that the estimate customer base is maintained over time. In terms of cost, it’s cost effective to maintain an existing customer than to acquire a new one. To this effect, every business enterprise ensures that the churn rate to minimized and also endeavor to identify factors that could be responsible for customer churn and addresses them accordingly.\nThis experimental project addresses the customer churn in a telecommunication company.\nDifferent classification models are considered in the modeling section using the tidymodels methodology in R."
  },
  {
    "objectID": "posts/telco-churn-tidymodels-post/index.html#data-wrangling",
    "href": "posts/telco-churn-tidymodels-post/index.html#data-wrangling",
    "title": "Telco Customer Churn",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nConvert character typed data to factor except the customerID field\n\nCode# convert character typed data to factor except the customerID\n\ntelco_df %<>%\n  select_if(is.character) %>% \n  mutate(across(c(where(is.character), -c(customerID)), as.factor))"
  },
  {
    "objectID": "posts/telco-churn-tidymodels-post/index.html#exploratory-data-analysis-of-the-dataset",
    "href": "posts/telco-churn-tidymodels-post/index.html#exploratory-data-analysis-of-the-dataset",
    "title": "Telco Customer Churn",
    "section": "Exploratory Data Analysis of the Dataset",
    "text": "Exploratory Data Analysis of the Dataset\n\nCodeglimpse(telco_df)\n\nRows: 7,043\nColumns: 17\n$ customerID       <chr> \"7590-VHVEG\", \"5575-GNVDE\", \"3668-QPYBK\", \"7795-CFOCW~\n$ gender           <fct> Female, Male, Male, Male, Female, Female, Male, Femal~\n$ Partner          <fct> Yes, No, No, No, No, No, No, No, Yes, No, Yes, No, Ye~\n$ Dependents       <fct> No, No, No, No, No, No, Yes, No, No, Yes, Yes, No, No~\n$ PhoneService     <fct> No, Yes, Yes, No, Yes, Yes, Yes, No, Yes, Yes, Yes, Y~\n$ MultipleLines    <fct> No phone service, No, No, No phone service, No, Yes, ~\n$ InternetService  <fct> DSL, DSL, DSL, DSL, Fiber optic, Fiber optic, Fiber o~\n$ OnlineSecurity   <fct> No, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Yes, No ~\n$ OnlineBackup     <fct> Yes, No, Yes, No, No, No, Yes, No, No, Yes, No, No in~\n$ DeviceProtection <fct> No, Yes, No, Yes, No, Yes, No, No, Yes, No, No, No in~\n$ TechSupport      <fct> No, No, No, Yes, No, No, No, No, Yes, No, No, No inte~\n$ StreamingTV      <fct> No, No, No, No, No, Yes, Yes, No, Yes, No, No, No int~\n$ StreamingMovies  <fct> No, No, No, No, No, Yes, No, No, Yes, No, No, No inte~\n$ Contract         <fct> Month-to-month, One year, Month-to-month, One year, M~\n$ PaperlessBilling <fct> Yes, No, Yes, No, Yes, Yes, Yes, No, Yes, No, Yes, No~\n$ PaymentMethod    <fct> Electronic check, Mailed check, Mailed check, Bank tr~\n$ Churn            <fct> No, No, Yes, No, Yes, Yes, No, No, Yes, No, No, No, N~\n\n\n\nCodereactable(telco_df, searchable = TRUE, filterable = TRUE, sortable = TRUE, pagination = TRUE)\n\n\n\n\n\n\n\nCode# brief data summary\n\nsummary(telco_df)\n\n  customerID           gender     Partner    Dependents PhoneService\n Length:7043        Female:3488   No :3641   No :4933   No : 682    \n Class :character   Male  :3555   Yes:3402   Yes:2110   Yes:6361    \n Mode  :character                                                   \n                                                                    \n          MultipleLines     InternetService             OnlineSecurity\n No              :3390   DSL        :2421   No                 :3498  \n No phone service: 682   Fiber optic:3096   No internet service:1526  \n Yes             :2971   No         :1526   Yes                :2019  \n                                                                      \n              OnlineBackup             DeviceProtection\n No                 :3088   No                 :3095   \n No internet service:1526   No internet service:1526   \n Yes                :2429   Yes                :2422   \n                                                       \n              TechSupport                StreamingTV  \n No                 :3473   No                 :2810  \n No internet service:1526   No internet service:1526  \n Yes                :2044   Yes                :2707  \n                                                      \n            StreamingMovies           Contract    PaperlessBilling\n No                 :2785   Month-to-month:3875   No :2872        \n No internet service:1526   One year      :1473   Yes:4171        \n Yes                :2732   Two year      :1695                   \n                                                                  \n                   PaymentMethod  Churn     \n Bank transfer (automatic):1544   No :5174  \n Credit card (automatic)  :1522   Yes:1869  \n Electronic check         :2365             \n Mailed check             :1612             \n\n\n\nCode# detailed summary\n\nDesc(telco_df)\n\n------------------------------------------------------------------------------ \nDescribe telco_df (tbl_df, tbl, data.frame):\n\ndata frame: 7043 obs. of  17 variables\n        7043 complete cases (100.0%)\n\n  Nr  ColName           Class      NAs  Levels                            \n  1   customerID        character  .                                      \n  2   gender            factor     .    (2): 1-Female, 2-Male             \n  3   Partner           factor     .    (2): 1-No, 2-Yes                  \n  4   Dependents        factor     .    (2): 1-No, 2-Yes                  \n  5   PhoneService      factor     .    (2): 1-No, 2-Yes                  \n  6   MultipleLines     factor     .    (3): 1-No, 2-No phone service,    \n                                        3-Yes                             \n  7   InternetService   factor     .    (3): 1-DSL, 2-Fiber optic, 3-No   \n  8   OnlineSecurity    factor     .    (3): 1-No, 2-No internet service, \n                                        3-Yes                             \n  9   OnlineBackup      factor     .    (3): 1-No, 2-No internet service, \n                                        3-Yes                             \n  10  DeviceProtection  factor     .    (3): 1-No, 2-No internet service, \n                                        3-Yes                             \n  11  TechSupport       factor     .    (3): 1-No, 2-No internet service, \n                                        3-Yes                             \n  12  StreamingTV       factor     .    (3): 1-No, 2-No internet service, \n                                        3-Yes                             \n  13  StreamingMovies   factor     .    (3): 1-No, 2-No internet service, \n                                        3-Yes                             \n  14  Contract          factor     .    (3): 1-Month-to-month, 2-One year,\n                                        3-Two year                        \n  15  PaperlessBilling  factor     .    (2): 1-No, 2-Yes                  \n  16  PaymentMethod     factor     .    (4): 1-Bank transfer (automatic), \n                                        2-Credit card (automatic),        \n                                        3-Electronic check, 4-Mailed check\n  17  Churn             factor     .    (2): 1-No, 2-Yes                  \n\n\n------------------------------------------------------------------------------ \n1 - customerID (character)\n\n  length      n    NAs unique levels  dupes\n   7'043  7'043      0  7'043  7'043      n\n         100.0%   0.0%                     \n\n         level  freq  perc  cumfreq  cumperc\n1   0002-ORFBO     1  0.0%        1     0.0%\n2   0003-MKNFE     1  0.0%        2     0.0%\n3   0004-TLHLJ     1  0.0%        3     0.0%\n4   0011-IGKFF     1  0.0%        4     0.1%\n5   0013-EXCHZ     1  0.0%        5     0.1%\n6   0013-MHZWF     1  0.0%        6     0.1%\n7   0013-SMEOE     1  0.0%        7     0.1%\n8   0014-BMAQU     1  0.0%        8     0.1%\n9   0015-UOCOJ     1  0.0%        9     0.1%\n10  0016-QLJIS     1  0.0%       10     0.1%\n11  0017-DINOC     1  0.0%       11     0.2%\n12  0017-IUDMW     1  0.0%       12     0.2%\n... etc.\n [list output truncated]\n\n\n\n\n\n------------------------------------------------------------------------------ \n2 - gender (factor - dichotomous)\n\n  length      n    NAs unique\n   7'043  7'043      0      2\n         100.0%   0.0%       \n\n         freq   perc  lci.95  uci.95'\nFemale  3'488  49.5%   48.4%   50.7%\nMale    3'555  50.5%   49.3%   51.6%\n\n' 95%-CI (Wilson)\n\n\n\n\n\n------------------------------------------------------------------------------ \n3 - Partner (factor - dichotomous)\n\n  length      n    NAs unique\n   7'043  7'043      0      2\n         100.0%   0.0%       \n\n      freq   perc  lci.95  uci.95'\nNo   3'641  51.7%   50.5%   52.9%\nYes  3'402  48.3%   47.1%   49.5%\n\n' 95%-CI (Wilson)\n\n\n\n\n\n------------------------------------------------------------------------------ \n4 - Dependents (factor - dichotomous)\n\n  length      n    NAs unique\n   7'043  7'043      0      2\n         100.0%   0.0%       \n\n      freq   perc  lci.95  uci.95'\nNo   4'933  70.0%   69.0%   71.1%\nYes  2'110  30.0%   28.9%   31.0%\n\n' 95%-CI (Wilson)\n\n\n\n\n\n------------------------------------------------------------------------------ \n5 - PhoneService (factor - dichotomous)\n\n  length      n    NAs unique\n   7'043  7'043      0      2\n         100.0%   0.0%       \n\n      freq   perc  lci.95  uci.95'\nNo     682   9.7%    9.0%   10.4%\nYes  6'361  90.3%   89.6%   91.0%\n\n' 95%-CI (Wilson)\n\n\n\n\n\n------------------------------------------------------------------------------ \n6 - MultipleLines (factor)\n\n  length      n    NAs unique levels  dupes\n   7'043  7'043      0      3      3      y\n         100.0%   0.0%                     \n\n              level   freq   perc  cumfreq  cumperc\n1                No  3'390  48.1%    3'390    48.1%\n2               Yes  2'971  42.2%    6'361    90.3%\n3  No phone service    682   9.7%    7'043   100.0%\n\n\n\n\n\n------------------------------------------------------------------------------ \n7 - InternetService (factor)\n\n  length      n    NAs unique levels  dupes\n   7'043  7'043      0      3      3      y\n         100.0%   0.0%                     \n\n         level   freq   perc  cumfreq  cumperc\n1  Fiber optic  3'096  44.0%    3'096    44.0%\n2          DSL  2'421  34.4%    5'517    78.3%\n3           No  1'526  21.7%    7'043   100.0%\n\n\n\n\n\n------------------------------------------------------------------------------ \n8 - OnlineSecurity (factor)\n\n  length      n    NAs unique levels  dupes\n   7'043  7'043      0      3      3      y\n         100.0%   0.0%                     \n\n                 level   freq   perc  cumfreq  cumperc\n1                   No  3'498  49.7%    3'498    49.7%\n2                  Yes  2'019  28.7%    5'517    78.3%\n3  No internet service  1'526  21.7%    7'043   100.0%\n\n\n\n\n\n------------------------------------------------------------------------------ \n9 - OnlineBackup (factor)\n\n  length      n    NAs unique levels  dupes\n   7'043  7'043      0      3      3      y\n         100.0%   0.0%                     \n\n                 level   freq   perc  cumfreq  cumperc\n1                   No  3'088  43.8%    3'088    43.8%\n2                  Yes  2'429  34.5%    5'517    78.3%\n3  No internet service  1'526  21.7%    7'043   100.0%\n\n\n\n\n\n------------------------------------------------------------------------------ \n10 - DeviceProtection (factor)\n\n  length      n    NAs unique levels  dupes\n   7'043  7'043      0      3      3      y\n         100.0%   0.0%                     \n\n                 level   freq   perc  cumfreq  cumperc\n1                   No  3'095  43.9%    3'095    43.9%\n2                  Yes  2'422  34.4%    5'517    78.3%\n3  No internet service  1'526  21.7%    7'043   100.0%\n\n\n\n\n\n------------------------------------------------------------------------------ \n11 - TechSupport (factor)\n\n  length      n    NAs unique levels  dupes\n   7'043  7'043      0      3      3      y\n         100.0%   0.0%                     \n\n                 level   freq   perc  cumfreq  cumperc\n1                   No  3'473  49.3%    3'473    49.3%\n2                  Yes  2'044  29.0%    5'517    78.3%\n3  No internet service  1'526  21.7%    7'043   100.0%\n\n\n\n\n\n------------------------------------------------------------------------------ \n12 - StreamingTV (factor)\n\n  length      n    NAs unique levels  dupes\n   7'043  7'043      0      3      3      y\n         100.0%   0.0%                     \n\n                 level   freq   perc  cumfreq  cumperc\n1                   No  2'810  39.9%    2'810    39.9%\n2                  Yes  2'707  38.4%    5'517    78.3%\n3  No internet service  1'526  21.7%    7'043   100.0%\n\n\n\n\n\n------------------------------------------------------------------------------ \n13 - StreamingMovies (factor)\n\n  length      n    NAs unique levels  dupes\n   7'043  7'043      0      3      3      y\n         100.0%   0.0%                     \n\n                 level   freq   perc  cumfreq  cumperc\n1                   No  2'785  39.5%    2'785    39.5%\n2                  Yes  2'732  38.8%    5'517    78.3%\n3  No internet service  1'526  21.7%    7'043   100.0%\n\n\n\n\n\n------------------------------------------------------------------------------ \n14 - Contract (factor)\n\n  length      n    NAs unique levels  dupes\n   7'043  7'043      0      3      3      y\n         100.0%   0.0%                     \n\n            level   freq   perc  cumfreq  cumperc\n1  Month-to-month  3'875  55.0%    3'875    55.0%\n2        Two year  1'695  24.1%    5'570    79.1%\n3        One year  1'473  20.9%    7'043   100.0%\n\n\n\n\n\n------------------------------------------------------------------------------ \n15 - PaperlessBilling (factor - dichotomous)\n\n  length      n    NAs unique\n   7'043  7'043      0      2\n         100.0%   0.0%       \n\n      freq   perc  lci.95  uci.95'\nNo   2'872  40.8%   39.6%   41.9%\nYes  4'171  59.2%   58.1%   60.4%\n\n' 95%-CI (Wilson)\n\n\n\n\n\n------------------------------------------------------------------------------ \n16 - PaymentMethod (factor)\n\n  length      n    NAs unique levels  dupes\n   7'043  7'043      0      4      4      y\n         100.0%   0.0%                     \n\n                       level   freq   perc  cumfreq  cumperc\n1           Electronic check  2'365  33.6%    2'365    33.6%\n2               Mailed check  1'612  22.9%    3'977    56.5%\n3  Bank transfer (automatic)  1'544  21.9%    5'521    78.4%\n4    Credit card (automatic)  1'522  21.6%    7'043   100.0%\n\n\n\n\n\n------------------------------------------------------------------------------ \n17 - Churn (factor - dichotomous)\n\n  length      n    NAs unique\n   7'043  7'043      0      2\n         100.0%   0.0%       \n\n      freq   perc  lci.95  uci.95'\nNo   5'174  73.5%   72.4%   74.5%\nYes  1'869  26.5%   25.5%   27.6%\n\n' 95%-CI (Wilson)\n\n\n\n\n\n\nCode# Gender Distribution\n\ntelco_df %>% \n  group_by(gender) %>%\n  summarise(Freq = n()) %>% \n  mutate(prop = Freq/sum(Freq)) %>% \n  filter(Freq != 0) %>% \n  \n  ggplot(mapping = aes(x = 2, y = prop, fill = gender))+\n  geom_bar(width = 1, color = \"white\", stat = \"identity\") +\n  xlim(0.5, 2.5) +\n  coord_polar(theta = \"y\", start = 0) +\n  theme_void() +\n  scale_y_continuous(labels = scales::percent) +\n  geom_text(aes(label = paste0(round(prop*100, 1), \"%\")), size = 4, position = position_stack(vjust = 0.5)) +\n  scale_fill_manual(values = c(\"#fc0394\",\"#03adfc\")) +\n  #theme(axis.text.x = element_text(angle = 90), legend.position = \"top\")+\n  labs(title = \"Customer Distribution by Gender\",\n       x = \"\",\n       y = \"\",\n       fill = \"\") +\n  theme(legend.position = \"top\") +\n   theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16))\n\n\n\n\n\nCode# Distribution of Churned Customer\n\n\ntelco_df %>% \n  mutate(Churn = case_when(Churn == \"No\" ~ \"Not Churned\",\n                            TRUE ~ \"Churned\")) %>% \n  group_by(Churn) %>%\n  summarise(Freq = n()) %>% \n  mutate(prop = Freq/sum(Freq)) %>% \n  filter(Freq != 0) %>% \n  \n  ggplot(mapping = aes(x = 2, y = prop, fill = Churn))+\n  geom_bar(width = 1, color = \"white\", stat = \"identity\") +\n  xlim(0.5, 2.5) +\n  coord_polar(theta = \"y\", start = 0) +\n  theme_void() +\n  scale_y_continuous(labels = scales::percent) +\n  geom_text(aes(label = paste0(round(prop*100, 1), \"%\")), size = 4, position = position_stack(vjust = 0.5)) +\n  scale_fill_manual(values = c('#FF0000', '#0000FF')) +\n  #theme(axis.text.x = element_text(angle = 90), legend.position = \"top\")+\n  labs(title = 'Distribution of Churned Customer',\n       x = \"\",\n       y = \"\",\n       fill = \"\") +\n  theme(legend.position = \"top\") +\n   theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16))\n\n\n\n\n\nCode# Payment Methods used by Customer\n\n\ntelco_df %>% \n  group_by(PaymentMethod) %>% \n  summarise(Count = n()) %>% \n  ggplot(aes(x = reorder(PaymentMethod, Count), y = Count)) +\n  geom_bar(stat = \"identity\", width = 0.3, fill = \"steelblue\", color = \"white\") +\n  labs(title = 'Payment Methods used by Customer',\n       x = \"Payment Method\") +\n  theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16),\n        axis.title = element_text(family = \"sans\", size = 10, face = \"plain\")) +\n  theme_clean() +\n  scale_y_continuous(labels = scales::comma) +\n  geom_text(aes(label = Count), size = 3.5)\n\n\n\n\n\nCode# Distribution of Customers by Internet Service\n\ntelco_df %>% \n  group_by(InternetService) %>% \n  summarise(Count = n()) %>% \n  ggplot(aes(x = reorder(InternetService, Count), y = Count)) +\n  geom_bar(stat = \"identity\", width = 0.3, fill = \"steelblue\", color = \"white\") +\n  labs(title = 'Distribution of Customers by Internet Service',\n       x = \"Internet Service\") +\n  theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16),\n        axis.title = element_text(family = \"sans\", size = 10, face = \"plain\")) +\n  theme_clean() +\n  scale_y_continuous(labels = scales::comma) +\n  geom_text(aes(label = Count), size = 3.5)\n\n\n\n\n\nCode# Distribution of Customer by Phone service\n\ntelco_df %>% \n  group_by(PhoneService) %>%\n  summarise(Freq = n()) %>% \n  mutate(prop = Freq/sum(Freq)) %>% \n  filter(Freq != 0) %>% \n  \n  ggplot(mapping = aes(x = 2, y = prop, fill = PhoneService))+\n  geom_bar(width = 1, color = \"white\", stat = \"identity\") +\n  xlim(0.5, 2.5) +\n  coord_polar(theta = \"y\", start = 0) +\n  theme_void() +\n  scale_y_continuous(labels = scales::percent) +\n  geom_text(aes(label = paste0(round(prop*100, 1), \"%\")), size = 4, position = position_stack(vjust = 0.5)) +\n  scale_fill_manual(values = c('#FF0000', '#0000FF')) +\n  #theme(axis.text.x = element_text(angle = 90), legend.position = \"top\")+\n  labs(title = 'Distribution of Customer by \\nPhone Service',\n       x = \"\",\n       y = \"\",\n       fill = \"\") +\n  theme(legend.position = \"top\") +\n   theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16))\n\n\n\n\n\nCode# Distribution of Customer by Paperless Billing\n\ntelco_df %>% \n  group_by(PaperlessBilling) %>%\n  summarise(Freq = n()) %>% \n  mutate(prop = Freq/sum(Freq)) %>% \n  filter(Freq != 0) %>% \n  \n  ggplot(mapping = aes(x = 2, y = prop, fill = PaperlessBilling))+\n  geom_bar(width = 1, color = \"white\", stat = \"identity\") +\n  xlim(0.5, 2.5) +\n  coord_polar(theta = \"y\", start = 0) +\n  theme_void() +\n  scale_y_continuous(labels = scales::percent) +\n  geom_text(aes(label = paste0(round(prop*100, 1), \"%\")), size = 4, position = position_stack(vjust = 0.5)) +\n  scale_fill_manual(values = c('#FF0000', '#0000FF')) +\n  #theme(axis.text.x = element_text(angle = 90), legend.position = \"top\")+\n  labs(title = 'Distribution of Customer by \\nPaperless Billing',\n       x = \"\",\n       y = \"\",\n       fill = \"\") +\n  theme(legend.position = \"top\") +\n   theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16))\n\n\n\n\n\nCode# Distribution of Customers by Contract\n\ntelco_df %>% \n  group_by(Contract) %>% \n  summarise(Count = n()) %>% \n  ggplot(aes(x = reorder(Contract, Count), y = Count)) +\n  geom_bar(stat = \"identity\", width = 0.3, fill = \"steelblue\", color = \"white\") +\n  labs(title = 'Distribution of Customers by Contract',\n       x = \"Contract Type\") +\n  theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16),\n        axis.title = element_text(family = \"sans\", size = 10, face = \"plain\")) +\n  theme_clean() +\n  scale_y_continuous(labels = scales::comma) +\n  geom_text(aes(label = Count), size = 3.5)\n\n\n\n\n\nCode# Distribution of Customer by Online Security\n\ntelco_df %>% \n  group_by(OnlineSecurity) %>%\n  summarise(Freq = n()) %>% \n  mutate(prop = Freq/sum(Freq)) %>% \n  filter(Freq != 0) %>% \n  \n  ggplot(mapping = aes(x = 2, y = prop, fill = OnlineSecurity))+\n  geom_bar(width = 1, color = \"white\", stat = \"identity\") +\n  xlim(0.5, 2.5) +\n  coord_polar(theta = \"y\", start = 0) +\n  theme_void() +\n  scale_y_continuous(labels = scales::percent) +\n  geom_text(aes(label = paste0(round(prop*100, 1), \"%\")), size = 4, position = position_stack(vjust = 0.5)) +\n  scale_fill_manual(values = c('#FF0000', 'tomato', 'darkorange')) +\n  #theme(axis.text.x = element_text(angle = 90), legend.position = \"top\")+\n  labs(title = 'Distribution of Customer by \\nOnline Security',\n       x = \"\",\n       y = \"\",\n       fill = \"\") +\n  theme(legend.position = \"top\") +\n   theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16))\n\n\n\n\n\nCode# Proportion of Churn by Gender\n\n\ntelco_df %>% \n  mutate(Churn = case_when(Churn == \"No\" ~ \"Not Churned\",\n                            TRUE ~ \"Churned\")) %>% \n  group_by(gender, Churn) %>%\n  summarise(Count = n()) %>% \n  mutate(Prop = Count/sum(Count)) %>% \n  ggplot(aes(x = reorder(gender, Prop), y = Prop, fill = Churn)) +\n  geom_bar(stat = \"identity\", width = 0.3, color = \"white\", position = \"fill\") +\n  labs(title = 'Proportion of Churn by Gender',\n       x = \"\",\n       y = \"\") +\n   scale_fill_manual(values = c('#FF0000', '#0000FF')) +\n  theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16),\n        axis.title = element_text(family = \"sans\", size = 10, face = \"plain\")) +\n  theme_clean() +\n  scale_y_continuous(labels = scales::percent) +\n  geom_text(aes(label = paste0(round(Prop*100,1),\"%\")), size = 3.5, position = position_fill(vjust = 0.5))\n\n\n\n\n\nCode# Proportion of Churn by PaymentMethod\n\n\ntelco_df %>% \n  mutate(Churn = case_when(Churn == \"No\" ~ \"Not Churned\",\n                            TRUE ~ \"Churned\")) %>% \n  group_by(PaymentMethod, Churn) %>%\n  summarise(Count = n()) %>% \n  mutate(Prop = Count/sum(Count)) %>% \n  ggplot(aes(x = reorder(PaymentMethod, Prop), y = Prop, fill = Churn)) +\n  geom_bar(stat = \"identity\", width = 0.5, color = \"white\", position = \"fill\") +\n  labs(title = 'Proportion of Churn by Payment Method',\n       x = \"\",\n       y = \"\") +\n   scale_fill_manual(values = c('#FF0000', '#0000FF')) +\n  theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16),\n        axis.title = element_text(family = \"sans\", size = 10, face = \"plain\")) +\n  theme_clean() +\n  scale_y_continuous(labels = scales::percent) +\n  geom_text(aes(label = paste0(round(Prop*100,1),\"%\")), size = 3.5, position = position_fill(vjust = 0.5))\n\n\n\n\n\nCode# Proportion of Churn by Contract Type\n\n\ntelco_df %>% \n  mutate(Churn = case_when(Churn == \"No\" ~ \"Not Churned\",\n                            TRUE ~ \"Churned\")) %>% \n  group_by(Contract, Churn) %>%\n  summarise(Count = n()) %>% \n  mutate(Prop = Count/sum(Count)) %>% \n  ggplot(aes(x = reorder(Contract, Prop), y = Prop, fill = Churn)) +\n  geom_bar(stat = \"identity\", width = 0.5, color = \"white\", position = \"fill\") +\n  labs(title = 'Proportion of Churn by Contract Type',\n       x = \"\",\n       y = \"\") +\n   scale_fill_manual(values = c('#FF0000', '#0000FF')) +\n  theme(title = element_text(family = \"Sans\", face = \"bold\", size = 16),\n        axis.title = element_text(family = \"sans\", size = 10, face = \"plain\")) +\n  theme_clean() +\n  scale_y_continuous(labels = scales::percent) +\n  geom_text(aes(label = paste0(round(Prop*100,1),\"%\")), size = 3.5, position = position_fill(vjust = 0.5))"
  },
  {
    "objectID": "posts/telco-churn-tidymodels-post/index.html#data-quality",
    "href": "posts/telco-churn-tidymodels-post/index.html#data-quality",
    "title": "Telco Customer Churn",
    "section": "Data Quality",
    "text": "Data Quality\nCheck dataframe for NAs\n\nCodeany(is.na(telco_df))\n\n[1] FALSE\n\n\n\nNo NA is found. The dataset is complete without any missing values.\n\n\nCode# split data to train and test set\n\nset.seed(1234)\n\nsplit <- telco_df %>% \n  select(-customerID) %>% \n  initial_split(prop = 0.75, strata = Churn) # 75% training set | 25% testing set\n\ndf_train <- split %>% \n  training()\n\ndf_test <- split %>% \n  testing()"
  },
  {
    "objectID": "posts/telco-churn-tidymodels-post/index.html#model-recipe",
    "href": "posts/telco-churn-tidymodels-post/index.html#model-recipe",
    "title": "Telco Customer Churn",
    "section": "Model Recipe",
    "text": "Model Recipe\n\nCoderec <- recipe(Churn ~ ., data = df_train)\n\n# add preprocessing\n\nprepro <- rec %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_other(all_nominal_predictors()) %>% \n  step_filter_missing(all_nominal_predictors(),threshold = 0) %>% \n  prep()\n\nprepro"
  },
  {
    "objectID": "posts/telco-churn-tidymodels-post/index.html#define-the-model-with-parsnip",
    "href": "posts/telco-churn-tidymodels-post/index.html#define-the-model-with-parsnip",
    "title": "Telco Customer Churn",
    "section": "Define the model with parsnip",
    "text": "Define the model with parsnip\n\nCode## Logistic Regression\n\nlr <- logistic_reg(\n  mode = \"classification\"\n) %>% \n  set_engine(\"glm\")\n\n\n## Nearest Neighbor\n\nknn <- nearest_neighbor(\n  mode = \"classification\"\n) %>% \n  set_engine(\"kknn\")\n\n## Random Forest\n\nrf <- rand_forest(mode = \"classification\") %>% \n  set_engine(\"ranger\", importance='impurity')\n\n## Gradient Boost\n\ngb <- boost_tree(mode = \"classification\") %>% \n  set_engine(\"xgboost\")"
  },
  {
    "objectID": "posts/telco-churn-tidymodels-post/index.html#define-models-workflow",
    "href": "posts/telco-churn-tidymodels-post/index.html#define-models-workflow",
    "title": "Telco Customer Churn",
    "section": "Define models workflow",
    "text": "Define models workflow\n\nCode## Logistic Regression\n\nlr_wf <- workflow() %>% \n  add_recipe(prepro) %>% \n  add_model(lr)\n\n\n## Nearest Neighbor\n\nknn_wf <- workflow() %>% \n  add_recipe(prepro) %>% \n  add_model(knn)\n\n## Random Forest\n\nrf_wf <- workflow() %>% \n  add_recipe(prepro) %>% \n  add_model(rf)\n\n\n## Gradient Boost\n\ngb_wf <- workflow() %>% \n  add_recipe(prepro) %>% \n  add_model(gb)"
  },
  {
    "objectID": "posts/telco-churn-tidymodels-post/index.html#obtaining-predictions",
    "href": "posts/telco-churn-tidymodels-post/index.html#obtaining-predictions",
    "title": "Telco Customer Churn",
    "section": "Obtaining Predictions",
    "text": "Obtaining Predictions\n\nCodeset.seed(1234)\n\n## Logistic Regression\n\nlr_pred <- lr_wf %>% \n  fit(df_train) %>% \n  predict(df_test) %>% \n  bind_cols(df_test)\n\n\n## Nearest Neighbor\n\nknn_pred <- knn_wf %>% \n  fit(df_train) %>% \n  predict(df_test) %>% \n  bind_cols(df_test)\n\n## Random Forest\n\nrf_pred <- rf_wf %>% \n  fit(df_train) %>% \n  predict(df_test) %>% \n  bind_cols(df_test)\n\n\n## Gradient Boost\n\ngb_pred <- gb_wf %>% \n  fit(df_train) %>% \n  predict(df_test) %>% \n  bind_cols(df_test)"
  },
  {
    "objectID": "posts/telco-churn-tidymodels-post/index.html#evaluating-model-performance",
    "href": "posts/telco-churn-tidymodels-post/index.html#evaluating-model-performance",
    "title": "Telco Customer Churn",
    "section": "Evaluating model performance",
    "text": "Evaluating model performance\n\n\nkap: Kappa\n\nsens: Sensitivity\n\nspec: Specificity\n\nf_meas: F1\n\nmcc: Matthews correlation coefficient\n\nLogistic Regression\n\nCodelr_pred %>% \n  conf_mat(truth = Churn, estimate = .pred_class) %>% \n  summary()\n\n# A tibble: 13 x 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.786\n 2 kap                  binary         0.410\n 3 sens                 binary         0.894\n 4 spec                 binary         0.487\n 5 ppv                  binary         0.828\n 6 npv                  binary         0.625\n 7 mcc                  binary         0.416\n 8 j_index              binary         0.381\n 9 bal_accuracy         binary         0.691\n10 detection_prevalence binary         0.793\n11 precision            binary         0.828\n12 recall               binary         0.894\n13 f_meas               binary         0.860\n\n\nNearest Neighbor\n\nCodeknn_pred %>% \n  conf_mat(truth = Churn, estimate = .pred_class) %>% \n  summary()\n\n# A tibble: 13 x 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.708\n 2 kap                  binary         0.122\n 3 sens                 binary         0.884\n 4 spec                 binary         0.220\n 5 ppv                  binary         0.758\n 6 npv                  binary         0.407\n 7 mcc                  binary         0.131\n 8 j_index              binary         0.104\n 9 bal_accuracy         binary         0.552\n10 detection_prevalence binary         0.856\n11 precision            binary         0.758\n12 recall               binary         0.884\n13 f_meas               binary         0.816\n\n\nRandom Forest\n\nCoderf_pred %>% \n  conf_mat(truth = Churn, estimate = .pred_class) %>% \n  summary()\n\n# A tibble: 13 x 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.780\n 2 kap                  binary         0.369\n 3 sens                 binary         0.911\n 4 spec                 binary         0.419\n 5 ppv                  binary         0.813\n 6 npv                  binary         0.630\n 7 mcc                  binary         0.382\n 8 j_index              binary         0.330\n 9 bal_accuracy         binary         0.665\n10 detection_prevalence binary         0.823\n11 precision            binary         0.813\n12 recall               binary         0.911\n13 f_meas               binary         0.859\n\n\nGradient Boost\n\nCodegb_pred %>% \n  conf_mat(truth = Churn, estimate = .pred_class) %>% \n  summary()\n\n# A tibble: 13 x 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.774\n 2 kap                  binary         0.367\n 3 sens                 binary         0.895\n 4 spec                 binary         0.440\n 5 ppv                  binary         0.815\n 6 npv                  binary         0.602\n 7 mcc                  binary         0.374\n 8 j_index              binary         0.335\n 9 bal_accuracy         binary         0.668\n10 detection_prevalence binary         0.806\n11 precision            binary         0.815\n12 recall               binary         0.895\n13 f_meas               binary         0.853\n\n\nThe random forest seems to be better off going by the sensitivity and the specificity metrics."
  },
  {
    "objectID": "posts/telco-churn-tidymodels-post/index.html#random-forest-roc-curve",
    "href": "posts/telco-churn-tidymodels-post/index.html#random-forest-roc-curve",
    "title": "Telco Customer Churn",
    "section": "Random Forest Roc Curve",
    "text": "Random Forest Roc Curve\n\nCode## Random Forest\n\nprob_preds <- rf_wf %>% \n  fit(df_train) %>% \n  predict(df_test, type = \"prob\") %>% \n  bind_cols(df_test)\n\n\nthreshold_df <- prob_preds %>% \n  roc_curve(truth = Churn, estimate = .pred_No)\n\nthreshold_df %>% \n  autoplot()\n\n\n\nCoderoc_auc(prob_preds, truth = Churn, estimate = .pred_No)\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.824"
  },
  {
    "objectID": "posts/telco-churn-tidymodels-post/index.html#variable-importance-plot",
    "href": "posts/telco-churn-tidymodels-post/index.html#variable-importance-plot",
    "title": "Telco Customer Churn",
    "section": "Variable Importance Plot",
    "text": "Variable Importance Plot\nRelative variable importance plot\n\nCodefinal_rf_model <-\n  rf_wf %>%\n  fit(data = df_train)\n\nfinal_rf_model\n\n== Workflow [trained] ==========================================================\nPreprocessor: Recipe\nModel: rand_forest()\n\n-- Preprocessor ----------------------------------------------------------------\n3 Recipe Steps\n\n* step_dummy()\n* step_other()\n* step_filter_missing()\n\n-- Model -----------------------------------------------------------------------\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, importance = ~\"impurity\",      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      5281 \nNumber of independent variables:  26 \nMtry:                             5 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1469603 \n\nCodefinal_rf_model %>% \n  pull_workflow_fit()\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, importance = ~\"impurity\",      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      5281 \nNumber of independent variables:  26 \nMtry:                             5 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1469603 \n\nCode## variable importance plot\n\nlibrary(vip)\n\nfinal_rf_model %>%\n  extract_fit_parsnip() %>%\n  vip()"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Olumide Oyalola",
    "section": "",
    "text": "An exploratory analysis on the global obesity prevalence. I used flexdashboard to present the analysis! An annual data from 1975 to 2014 was explored. …. Continue reading"
  },
  {
    "objectID": "projects.html#us-energy-analysis",
    "href": "projects.html#us-energy-analysis",
    "title": "Olumide Oyalola",
    "section": "US Energy Analysis",
    "text": "US Energy Analysis\n\n\n\n\n\n\nExploratory analysis of the annual US crude oil import prices … Link to the flexdashboard"
  }
]